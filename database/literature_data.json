{
  "metadata": {
    "research_topic": "multi-agent RL behavior ",
    "generated_at": "2025-04-04T17:04:03.995340",
    "source": "arXiv"
  },
  "sub_topics": {
    "traffic": [
      {
        "title": "Learning Realistic Traffic Agents in Closed-loop",
        "arxiv_id": "2311.01394v1",
        "published": "2023-11-02",
        "summary": "Realistic traffic simulation is crucial for developing self-driving software\nin a safe and scalable manner prior to real-world deployment. Typically,\nimitation learning (IL) is used to learn human-like traffic agents directly\nfrom real-world observations collected offline, but without explicit\nspecification of traffic rules, agents trained from IL alone frequently display\nunrealistic infractions like collisions and driving off the road. This problem\nis exacerbated in out-of-distribution and long-tail scenarios. On the other\nhand, reinforcement learning (RL) can train traffic agents to avoid\ninfractions, but using RL alone results in unhuman-like driving behaviors. We\npropose Reinforcing Traffic Rules (RTR), a holistic closed-loop learning\nobjective to match expert demonstrations under a traffic compliance constraint,\nwhich naturally gives rise to a joint IL + RL approach, obtaining the best of\nboth worlds. Our method learns in closed-loop simulations of both nominal\nscenarios from real-world datasets as well as procedurally generated long-tail\nscenarios. Our experiments show that RTR learns more realistic and\ngeneralizable traffic simulation policies, achieving significantly better\ntradeoffs between human-like driving and traffic compliance in both nominal and\nlong-tail scenarios. Moreover, when used as a data generation tool for training\nprediction models, our learned traffic policy leads to considerably improved\ndownstream prediction metrics compared to baseline traffic agents. For more\ninformation, visit the project website: https://waabi.ai/rtr\nLink: http://arxiv.org/abs/2311.01394v1",
        "link": "http://arxiv.org/abs/2311.01394v1"
      }
    ],
    "CPS": [
      {
        "title": "Provably Efficient Action-Manipulation Attack Against Continuous Reinforcement Learning",
        "arxiv_id": "2411.13116v1",
        "published": "2024-11-20",
        "summary": "Manipulating the interaction trajectories between the intelligent agent and\nthe environment can control the agent's training and behavior, exposing the\npotential vulnerabilities of reinforcement learning (RL). For example, in\nCyber-Physical Systems (CPS) controlled by RL, the attacker can manipulate the\nactions of the adopted RL to other actions during the training phase, which\nwill lead to bad consequences. Existing work has studied action-manipulation\nattacks in tabular settings, where the states and actions are discrete. As seen\nin many up-and-coming RL applications, such as autonomous driving, continuous\naction space is widely accepted, however, its action-manipulation attacks have\nnot been thoroughly investigated yet. In this paper, we consider this crucial\nproblem in both white-box and black-box scenarios. Specifically, utilizing the\nknowledge derived exclusively from trajectories, we propose a black-box attack\nalgorithm named LCBT, which uses the Monte Carlo tree search method for\nefficient action searching and manipulation. Additionally, we demonstrate that\nfor an agent whose dynamic regret is sub-linearly related to the total number\nof steps, LCBT can teach the agent to converge to target policies with only\nsublinear attack cost, i.e., $O\\left(\\mathcal{R}(T) + MH^3K^E\\log\n(MT)\\right)(0<E<1)$, where $H$ is the number of steps per episode, $K$ is the\ntotal number of episodes, $T=KH$ is the total number of steps, $M$ is the\nnumber of subspaces divided in the state space, and $\\mathcal{R}(T)$ is the\nbound of the RL algorithm's regret. We conduct our proposed attack methods on\nthree aggressive algorithms: DDPG, PPO, and TD3 in continuous settings, which\nshow a promising attack performance.\nLink: http://arxiv.org/abs/2411.13116v1",
        "link": "http://arxiv.org/abs/2411.13116v1"
      }
    ],
    " Cooperative vs. competitive behaviors": [
      {
        "title": "Evaluating Collaborative Autonomy in Opposed Environments using Maritime Capture-the-Flag Competitions",
        "arxiv_id": "2404.17038v1",
        "published": "2024-04-25",
        "summary": "The objective of this work is to evaluate multi-agent artificial intelligence\nmethods when deployed on teams of unmanned surface vehicles (USV) in an\nadversarial environment. Autonomous agents were evaluated in real-world\nscenarios using the Aquaticus test-bed, which is a Capture-the-Flag (CTF) style\ncompetition involving teams of USV systems. Cooperative teaming algorithms of\nvarious foundations in behavior-based optimization and deep reinforcement\nlearning (RL) were deployed on these USV systems in two versus two teams and\ntested against each other during a competition period in the fall of 2023. Deep\nreinforcement learning applied to USV agents was achieved via the Pyquaticus\ntest bed, a lightweight gymnasium environment that allows simulated CTF\ntraining in a low-level environment. The results of the experiment demonstrate\nthat rule-based cooperation for behavior-based agents outperformed those\ntrained in Deep-reinforcement learning paradigms as implemented in these\ncompetitions. Further integration of the Pyquaticus gymnasium environment for\nRL with MOOS-IvP in terms of configuration and control schema will allow for\nmore competitive CTF games in future studies. As the development of\nexperimental deep RL methods continues, the authors expect that the competitive\ngap between behavior-based autonomy and deep RL will be reduced. As such, this\nreport outlines the overall competition, methods, and results with an emphasis\non future works such as reward shaping and sim-to-real methodologies and\nextending rule-based cooperation among agents to react to safety and security\nevents in accordance with human experts intent/rules for executing safety and\nsecurity processes.\nLink: http://arxiv.org/abs/2404.17038v1",
        "link": "http://arxiv.org/abs/2404.17038v1"
      }
    ],
    " Coordination without communication": [
      {
        "title": "Abstracted Trajectory Visualization for Explainability in Reinforcement Learning",
        "arxiv_id": "2402.07928v1",
        "published": "2024-02-05",
        "summary": "Explainable AI (XAI) has demonstrated the potential to help reinforcement\nlearning (RL) practitioners to understand how RL models work. However, XAI for\nusers who do not have RL expertise (non-RL experts), has not been studied\nsufficiently. This results in a difficulty for the non-RL experts to\nparticipate in the fundamental discussion of how RL models should be designed\nfor an incoming society where humans and AI coexist. Solving such a problem\nwould enable RL experts to communicate with the non-RL experts in producing\nmachine learning solutions that better fit our society. We argue that\nabstracted trajectories, that depicts transitions between the major states of\nthe RL model, will be useful for non-RL experts to build a mental model of the\nagents. Our early results suggest that by leveraging a visualization of the\nabstracted trajectories, users without RL expertise are able to infer the\nbehavior patterns of RL.\nLink: http://arxiv.org/abs/2402.07928v1",
        "link": "http://arxiv.org/abs/2402.07928v1"
      }
    ],
    " Stability and convergence guarantees": [
      {
        "title": "Regularization Guarantees Generalization in Bayesian Reinforcement Learning through Algorithmic Stability",
        "arxiv_id": "2109.11792v1",
        "published": "2021-09-24",
        "summary": "In the Bayesian reinforcement learning (RL) setting, a prior distribution\nover the unknown problem parameters -- the rewards and transitions -- is\nassumed, and a policy that optimizes the (posterior) expected return is sought.\nA common approximation, which has been recently popularized as meta-RL, is to\ntrain the agent on a sample of $N$ problem instances from the prior, with the\nhope that for large enough $N$, good generalization behavior to an unseen test\ninstance will be obtained. In this work, we study generalization in Bayesian RL\nunder the probably approximately correct (PAC) framework, using the method of\nalgorithmic stability. Our main contribution is showing that by adding\nregularization, the optimal policy becomes stable in an appropriate sense. Most\nstability results in the literature build on strong convexity of the\nregularized loss -- an approach that is not suitable for RL as Markov decision\nprocesses (MDPs) are not convex. Instead, building on recent results of fast\nconvergence rates for mirror descent in regularized MDPs, we show that\nregularized MDPs satisfy a certain quadratic growth criterion, which is\nsufficient to establish stability. This result, which may be of independent\ninterest, allows us to study the effect of regularization on generalization in\nthe Bayesian RL setting.\nLink: http://arxiv.org/abs/2109.11792v1",
        "link": "http://arxiv.org/abs/2109.11792v1"
      }
    ],
    " Emergent communication strategies": [
      {
        "title": "How Can Creativity Occur in Multi-Agent Systems?",
        "arxiv_id": "2111.14310v1",
        "published": "2021-11-29",
        "summary": "Complex systems show how surprising and beautiful phenomena can emerge from\nstructures or agents following simple rules. With the recent success of deep\nreinforcement learning (RL), a natural path forward would be to use the\ncapabilities of multiple deep RL agents to produce emergent behavior of greater\nbenefit and sophistication. In general, this has proved to be an unreliable\nstrategy without significant computation due to the difficulties inherent in\nmulti-agent RL training. In this paper, we propose some criteria for creativity\nin multi-agent RL. We hope this proposal will give artists applying multi-agent\nRL a starting point, and provide a catalyst for further investigation guided by\nphilosophical discussion.\nLink: http://arxiv.org/abs/2111.14310v1",
        "link": "http://arxiv.org/abs/2111.14310v1"
      }
    ],
    " Scalability in multi-agent systems": [
      {
        "title": "Survey on Strategic Mining in Blockchain: A Reinforcement Learning Approach",
        "arxiv_id": "2502.17307v2",
        "published": "2025-02-24",
        "summary": "Strategic mining attacks, such as selfish mining, exploit blockchain\nconsensus protocols by deviating from honest behavior to maximize rewards.\nMarkov Decision Process (MDP) analysis faces scalability challenges in modern\ndigital economics, including blockchain. To address these limitations,\nreinforcement learning (RL) provides a scalable alternative, enabling adaptive\nstrategy optimization in complex dynamic environments.\n  In this survey, we examine RL's role in strategic mining analysis, comparing\nit to MDP-based approaches. We begin by reviewing foundational MDP models and\ntheir limitations, before exploring RL frameworks that can learn near-optimal\nstrategies across various protocols. Building on this analysis, we compare RL\ntechniques and their effectiveness in deriving security thresholds, such as the\nminimum attacker power required for profitable attacks. Expanding the\ndiscussion further, we classify consensus protocols and propose open\nchallenges, such as multi-agent dynamics and real-world validation.\n  This survey highlights the potential of reinforcement learning (RL) to\naddress the challenges of selfish mining, including protocol design, threat\ndetection, and security analysis, while offering a strategic roadmap for\nresearchers in decentralized systems and AI-driven analytics.\nLink: http://arxiv.org/abs/2502.17307v2",
        "link": "http://arxiv.org/abs/2502.17307v2"
      }
    ],
    " Opponent modeling techniques": [
      {
        "title": "Self-Confirming Transformer for Belief-Conditioned Adaptation in Offline Multi-Agent Reinforcement Learning",
        "arxiv_id": "2310.04579v2",
        "published": "2023-10-06",
        "summary": "Offline reinforcement learning (RL) suffers from the distribution shift\nbetween the offline dataset and the online environment. In multi-agent RL\n(MARL), this distribution shift may arise from the nonstationary opponents in\nthe online testing who display distinct behaviors from those recorded in the\noffline dataset. Hence, the key to the broader deployment of offline MARL is\nthe online adaptation to nonstationary opponents. Recent advances in foundation\nmodels, e.g., large language models, have demonstrated the generalization\nability of the transformer, an emerging neural network architecture, in\nsequence modeling, of which offline RL is a special case. One naturally wonders\n\\textit{whether offline-trained transformer-based RL policies adapt to\nnonstationary opponents online}. We propose a novel auto-regressive training to\nequip transformer agents with online adaptability based on the idea of\nself-augmented pre-conditioning. The transformer agent first learns offline to\npredict the opponent's action based on past observations. When deployed online,\nsuch a fictitious opponent play, referred to as the belief, is fed back to the\ntransformer, together with other environmental feedback, to generate future\nactions conditional on the belief. Motivated by self-confirming equilibrium in\ngame theory, the training loss consists of belief consistency loss, requiring\nthe beliefs to match the opponent's actual actions and best response loss,\nmandating the agent to behave optimally under the belief. We evaluate the\nonline adaptability of the proposed self-confirming transformer (SCT) in a\nstructured environment, iterated prisoner's dilemma games, to demonstrate SCT's\nbelief consistency and equilibrium behaviors as well as more involved\nmulti-particle environments to showcase its superior performance against\nnonstationary opponents over prior transformers and offline MARL baselines.\nLink: http://arxiv.org/abs/2310.04579v2",
        "link": "http://arxiv.org/abs/2310.04579v2"
      }
    ]
  }
}