{
  "metadata": {
    "research_topic": "Tools for the literature review. Similar to automated systematic review.",
    "generated_at": "2025-03-23T02:09:04.514684",
    "source": "arXiv"
  },
  "sub_topics": {
    "AI-powered literature screening": [
      {
        "title": "Streamlining Systematic Reviews: A Novel Application of Large Language Models",
        "arxiv_id": "2412.15247v1",
        "published": "2024-12-14",
        "summary": "Systematic reviews (SRs) are essential for evidence-based guidelines but are\noften limited by the time-consuming nature of literature screening. We propose\nand evaluate an in-house system based on Large Language Models (LLMs) for\nautomating both title/abstract and full-text screening, addressing a critical\ngap in the literature. Using a completed SR on Vitamin D and falls (14,439\narticles), the LLM-based system employed prompt engineering for title/abstract\nscreening and Retrieval-Augmented Generation (RAG) for full-text screening. The\nsystem achieved an article exclusion rate (AER) of 99.5%, specificity of 99.6%,\na false negative rate (FNR) of 0%, and a negative predictive value (NPV) of\n100%. After screening, only 78 articles required manual review, including all\n20 identified by traditional methods, reducing manual screening time by 95.5%.\nFor comparison, Rayyan, a commercial tool for title/abstract screening,\nachieved an AER of 72.1% and FNR of 5% when including articles Rayyan\nconsidered as undecided or likely to include. Lowering Rayyan's inclusion\nthresholds improved FNR to 0% but increased screening time. By addressing both\nscreening phases, the LLM-based system significantly outperformed Rayyan and\ntraditional methods, reducing total screening time to 25.5 hours while\nmaintaining high accuracy. These findings highlight the transformative\npotential of LLMs in SR workflows by offering a scalable, efficient, and\naccurate solution, particularly for the full-text screening phase, which has\nlacked automation tools.\nLink: http://arxiv.org/abs/2412.15247v1",
        "link": "http://arxiv.org/abs/2412.15247v1"
      },
      {
        "title": "CRUISE-Screening: Living Literature Reviews Toolbox",
        "arxiv_id": "2309.01684v1",
        "published": "2023-09-04",
        "summary": "Keeping up with research and finding related work is still a time-consuming\ntask for academics. Researchers sift through thousands of studies to identify a\nfew relevant ones. Automation techniques can help by increasing the efficiency\nand effectiveness of this task. To this end, we developed CRUISE-Screening, a\nweb-based application for conducting living literature reviews - a type of\nliterature review that is continuously updated to reflect the latest research\nin a particular field. CRUISE-Screening is connected to several search engines\nvia an API, which allows for updating the search results periodically.\nMoreover, it can facilitate the process of screening for relevant publications\nby using text classification and question answering models. CRUISE-Screening\ncan be used both by researchers conducting literature reviews and by those\nworking on automating the citation screening process to validate their\nalgorithms. The application is open-source:\nhttps://github.com/ProjectDoSSIER/cruise-screening, and a demo is available\nunder this URL: https://citation-screening.ec.tuwien.ac.at. We discuss the\nlimitations of our tool in Appendix A.\nLink: http://arxiv.org/abs/2309.01684v1",
        "link": "http://arxiv.org/abs/2309.01684v1"
      },
      {
        "title": "CSMeD: Bridging the Dataset Gap in Automated Citation Screening for Systematic Literature Reviews",
        "arxiv_id": "2311.12474v1",
        "published": "2023-11-21",
        "summary": "Systematic literature reviews (SLRs) play an essential role in summarising,\nsynthesising and validating scientific evidence. In recent years, there has\nbeen a growing interest in using machine learning techniques to automate the\nidentification of relevant studies for SLRs. However, the lack of standardised\nevaluation datasets makes comparing the performance of such automated\nliterature screening systems difficult. In this paper, we analyse the citation\nscreening evaluation datasets, revealing that many of the available datasets\nare either too small, suffer from data leakage or have limited applicability to\nsystems treating automated literature screening as a classification task, as\nopposed to, for example, a retrieval or question-answering task. To address\nthese challenges, we introduce CSMeD, a meta-dataset consolidating nine\npublicly released collections, providing unified access to 325 SLRs from the\nfields of medicine and computer science. CSMeD serves as a comprehensive\nresource for training and evaluating the performance of automated citation\nscreening models. Additionally, we introduce CSMeD-FT, a new dataset designed\nexplicitly for evaluating the full text publication screening task. To\ndemonstrate the utility of CSMeD, we conduct experiments and establish\nbaselines on new datasets.\nLink: http://arxiv.org/abs/2311.12474v1",
        "link": "http://arxiv.org/abs/2311.12474v1"
      }
    ],
    "Automated data extraction tools": [
      {
        "title": "Automated data processing and feature engineering for deep learning and big data applications: a survey",
        "arxiv_id": "2403.11395v2",
        "published": "2024-03-18",
        "summary": "Modern approach to artificial intelligence (AI) aims to design algorithms\nthat learn directly from data. This approach has achieved impressive results\nand has contributed significantly to the progress of AI, particularly in the\nsphere of supervised deep learning. It has also simplified the design of\nmachine learning systems as the learning process is highly automated. However,\nnot all data processing tasks in conventional deep learning pipelines have been\nautomated. In most cases data has to be manually collected, preprocessed and\nfurther extended through data augmentation before they can be effective for\ntraining. Recently, special techniques for automating these tasks have emerged.\nThe automation of data processing tasks is driven by the need to utilize large\nvolumes of complex, heterogeneous data for machine learning and big data\napplications. Today, end-to-end automated data processing systems based on\nautomated machine learning (AutoML) techniques are capable of taking raw data\nand transforming them into useful features for Big Data tasks by automating all\nintermediate processing stages. In this work, we present a thorough review of\napproaches for automating data processing tasks in deep learning pipelines,\nincluding automated data preprocessing--e.g., data cleaning, labeling, missing\ndata imputation, and categorical data encoding--as well as data augmentation\n(including synthetic data generation using generative AI methods) and feature\nengineering--specifically, automated feature extraction, feature construction\nand feature selection. In addition to automating specific data processing\ntasks, we discuss the use of AutoML methods and tools to simultaneously\noptimize all stages of the machine learning pipeline.\nLink: http://arxiv.org/abs/2403.11395v2",
        "link": "http://arxiv.org/abs/2403.11395v2"
      },
      {
        "title": "Automated split Hopkinson pressure bar for high throughput dynamic experiments",
        "arxiv_id": "2502.02729v1",
        "published": "2025-02-04",
        "summary": "Designing novel materials for impact applications and predicting material\nbehavior using data-driven techniques require extensive datasets, which can be\nchallenging to obtain through conventional experimental methods. While high\nthroughput characterization is feasible at the sample micro and nanoscale,\nthere remains a critical need for an experimental tool capable of facilitating\nhigh-throughput measurements at the macroscale. This paper presents the design\nand development of a fully automated split Hopkinson pressure bar (SHPB) with\nfull-field diagnostics for performing dynamic compression experiments on\ndifferent materials. The automated SHPB experiments consist of four main\ncomponents: 1) striker launch and retrieval system, 2) bar repositioning\nmechanism, 3) automated sample placement system, and 4) diagnostics. Each of\nthese systems employs various electromechanical devices and actuators\nprogrammed to work collaboratively to automate the SHPB experimental setup. A\ndata analysis tool has been developed to automate the post-processing of the\ndata obtained from the setup. The setup also incorporates automated high-speed\nimaging, enabling full-field strain measurement capabilities. To benchmark the\nsetup, fully automated dynamic compression experiments were conducted on 45\nCopper 101 samples and 20 3D-printed resin samples. The stress-strain curve was\nextracted from the raw data using the automated data analysis tool. The\nefficiency of the data analysis tool is validated by comparing the data with\nexisting data analysis tools for SHPB. The stress-strain response obtained from\nthe copper experiments aligned well with existing data on copper. Furthermore,\nthe full-field strain measurements in experiments show strain values comparable\nto those of the strain gauge measurements. This advancement has enabled dynamic\ncompression experiments on materials at a rate of 60 samples per hour.\nLink: http://arxiv.org/abs/2502.02729v1",
        "link": "http://arxiv.org/abs/2502.02729v1"
      },
      {
        "title": "MedPromptExtract (Medical Data Extraction Tool): Anonymization and Hi-fidelity Automated data extraction using NLP and prompt engineering",
        "arxiv_id": "2405.02664v3",
        "published": "2024-05-04",
        "summary": "Introduction: The labour-intensive nature of data extraction from sources\nlike discharge summaries (DS) poses significant obstacles to the digitisation\nof medical records particularly for low- and middle-income countries (LMICs).\nIn this paper we present a completely automated method MedPromptExtract to\nefficiently extract data from DS while maintaining confidentiality. Methods:\nThe source of data was Discharge Summaries (DS) from Kokilaben Dhirubhai Ambani\nHospital (KDAH) of patients having Acute Kidney Injury (AKI). A pre-existing\ntool EIGEN which leverages semi-supervised learning techniques for\nhigh-fidelity information extraction was used to anonymize the DS, Natural\nLanguage Processing (NLP) was used to extract data from regular fields. We used\nPrompt Engineering and Large Language Model(LLM) to extract custom clinical\ninformation from free flowing text describing the patients stay in the\nhospital. Twelve features associated with occurrence of AKI were extracted. The\nLLM responses were validated against clinicians annotations. Results: The\nMedPromptExtracttool first subjected DS to the anonymization pipeline which\ntook three seconds per summary. Successful anonymization was verified by\nclinicians, thereafter NLP pipeline extracted structured text from the\nanonymized pdfs at the rate of 0.2 seconds per summary with 100%\naccuracy.Finally DS were analysed by the LLM pipeline using Gemini Pro for the\ntwelve features. Accuracy metrics were calculated by comparing model responses\nto clinicians annotations with seven features achieving AUCs above 0.9,\nindicating high fidelity of the extraction process. Conclusion:\nMedPromptExtract serves as an automated adaptable tool for efficient data\nextraction from medical records with a dynamic user interface. Keywords:\nDigitizing Medical Records, Automated Anonymisation, Information Retrieval,\nLarge Language Models, Prompt Engineering\nLink: http://arxiv.org/abs/2405.02664v3",
        "link": "http://arxiv.org/abs/2405.02664v3"
      }
    ],
    "Citation network analysis software": [
      {
        "title": "Generative Dynamics of Supreme Court Citations: Analysis with a New Statistical Network Model",
        "arxiv_id": "2101.07197v1",
        "published": "2021-01-15",
        "summary": "The significance and influence of US Supreme Court majority opinions derive\nin large part from opinions' roles as precedents for future opinions. A growing\nbody of literature seeks to understand what drives the use of opinions as\nprecedents through the study of Supreme Court case citation patterns. We raise\ntwo limitations of existing work on Supreme Court citations. First, dyadic\ncitations are typically aggregated to the case level before they are analyzed.\nSecond, citations are treated as if they arise independently. We present a\nmethodology for studying citations between Supreme Court opinions at the dyadic\nlevel, as a network, that overcomes these limitations. This methodology -- the\ncitation exponential random graph model, for which we provide user-friendly\nsoftware -- enables researchers to account for the effects of case\ncharacteristics and complex forms of network dependence in citation formation.\nWe then analyze a network that includes all Supreme Court cases decided between\n1950 and 2015. We find evidence for dependence processes, including\nreciprocity, transitivity, and popularity. The dependence effects are as\nsubstantively and statistically significant as the effects of exogenous\ncovariates, indicating that models of Supreme Court citation should incorporate\nboth the effects of case characteristics and the structure of past citations.\nLink: http://arxiv.org/abs/2101.07197v1",
        "link": "http://arxiv.org/abs/2101.07197v1"
      },
      {
        "title": "Characterizing health informatics journals by subject-level dependencies: a citation network analysis",
        "arxiv_id": "1807.08841v2",
        "published": "2018-07-23",
        "summary": "Citation network analysis has become one of methods to study how scientific\nknowledge flows from one domain to another. Health informatics is a\nmultidisciplinary field that includes social science, software engineering,\nbehavioral science, medical science and others. In this study, we perform an\nanalysis of citation statistics from health informatics journals using data set\nextracted from CrossRef. For each health informatics journal, we extract the\nnumber of citations from/to studies related to computer science,\nmedicine/clinical medicine and other fields, including the number of\nself-citations from the health informatics journal. With a similar number of\narticles used in our analysis, we show that the Journal of the American Medical\nInformatics Association (JAMIA) has more in-citations than the Journal of\nMedical Internet Research (JMIR); while JMIR has a higher number of\nout-citations and self-citations. We also show that JMIR cites more articles\nfrom health informatics journals and medicine related journals. In addition,\nthe Journal of Medical Systems (JMS) cites more articles from computer science\njournals compared with other health informatics journals included in our\nanalysis.\nLink: http://arxiv.org/abs/1807.08841v2",
        "link": "http://arxiv.org/abs/1807.08841v2"
      },
      {
        "title": "Reference Publication Year Spectroscopy (RPYS) in practice: A software tutorial",
        "arxiv_id": "2109.00969v3",
        "published": "2021-09-02",
        "summary": "In course of the organization of Workshop III entitled \"Cited References\nAnalysis Using CRExplorer\" at the International Conference of the International\nSociety for Scientometrics and Informetrics (ISSI2021), we have prepared three\nreference publication year spectroscopy (RPYS) analyses: (i) papers published\nin Journal of Informetrics; (ii) papers regarding the topic altmetrics; and\n(iii) papers published by Ludo Waltman (we selected this researcher since he\nreceived the Derek de Solla Price Memorial Medal during the ISSI2021\nconference). The first RPYS analysis has been presented live at the workshop\nand the second and third RPYS analyses have been left to the participants for\nundertaking after the workshop. Here, we present the results for all three RPYS\nanalyses. The three analyses have shown quite different seminal papers with a\nfew overlaps. Many of the foundational papers in the field of scientometrics\n(e.g., distributions of publications and citations, citation network and\nco-citation analyses, and citation analysis with the aim of impact measurement\nand research evaluation) were retrieved as seminal papers of the papers\npublished in Journal of Informetrics. Mainly papers with discussions of the\ndeficiencies of citation-based impact measurements and comparisons between\naltmetrics and citations were retrieved as seminal papers of the topic\naltmetrics. The RPYS analysis of the paper set published by Ludo Waltman mainly\nretrieved papers about network analyses, citation relations, and citation\nimpact measurement.\nLink: http://arxiv.org/abs/2109.00969v3",
        "link": "http://arxiv.org/abs/2109.00969v3"
      }
    ],
    "Bias detection in reviews": [
      {
        "title": "The Media Bias Taxonomy: A Systematic Literature Review on the Forms and Automated Detection of Media Bias",
        "arxiv_id": "2312.16148v3",
        "published": "2023-12-26",
        "summary": "The way the media presents events can significantly affect public perception,\nwhich in turn can alter people's beliefs and views. Media bias describes a\none-sided or polarizing perspective on a topic. This article summarizes the\nresearch on computational methods to detect media bias by systematically\nreviewing 3140 research papers published between 2019 and 2022. To structure\nour review and support a mutual understanding of bias across research domains,\nwe introduce the Media Bias Taxonomy, which provides a coherent overview of the\ncurrent state of research on media bias from different perspectives. We show\nthat media bias detection is a highly active research field, in which\ntransformer-based classification approaches have led to significant\nimprovements in recent years. These improvements include higher classification\naccuracy and the ability to detect more fine-granular types of bias. However,\nwe have identified a lack of interdisciplinarity in existing projects, and a\nneed for more awareness of the various types of media bias to support\nmethodologically thorough performance evaluations of media bias detection\nsystems. Concluding from our analysis, we see the integration of recent machine\nlearning advancements with reliable and diverse bias assessment strategies from\nother research areas as the most promising area for future research\ncontributions in the field.\nLink: http://arxiv.org/abs/2312.16148v3",
        "link": "http://arxiv.org/abs/2312.16148v3"
      },
      {
        "title": "Introducing MBIB -- the first Media Bias Identification Benchmark Task and Dataset Collection",
        "arxiv_id": "2304.13148v1",
        "published": "2023-04-25",
        "summary": "Although media bias detection is a complex multi-task problem, there is, to\ndate, no unified benchmark grouping these evaluation tasks. We introduce the\nMedia Bias Identification Benchmark (MBIB), a comprehensive benchmark that\ngroups different types of media bias (e.g., linguistic, cognitive, political)\nunder a common framework to test how prospective detection techniques\ngeneralize. After reviewing 115 datasets, we select nine tasks and carefully\npropose 22 associated datasets for evaluating media bias detection techniques.\nWe evaluate MBIB using state-of-the-art Transformer techniques (e.g., T5,\nBART). Our results suggest that while hate speech, racial bias, and gender bias\nare easier to detect, models struggle to handle certain bias types, e.g.,\ncognitive and political bias. However, our results show that no single\ntechnique can outperform all the others significantly. We also find an uneven\ndistribution of research interest and resource allocation to the individual\ntasks in media bias. A unified benchmark encourages the development of more\nrobust systems and shifts the current paradigm in media bias detection\nevaluation towards solutions that tackle not one but multiple media bias types\nsimultaneously.\nLink: http://arxiv.org/abs/2304.13148v1",
        "link": "http://arxiv.org/abs/2304.13148v1"
      },
      {
        "title": "Anatomizing Bias in Facial Analysis",
        "arxiv_id": "2112.06522v1",
        "published": "2021-12-13",
        "summary": "Existing facial analysis systems have been shown to yield biased results\nagainst certain demographic subgroups. Due to its impact on society, it has\nbecome imperative to ensure that these systems do not discriminate based on\ngender, identity, or skin tone of individuals. This has led to research in the\nidentification and mitigation of bias in AI systems. In this paper, we\nencapsulate bias detection/estimation and mitigation algorithms for facial\nanalysis. Our main contributions include a systematic review of algorithms\nproposed for understanding bias, along with a taxonomy and extensive overview\nof existing bias mitigation algorithms. We also discuss open challenges in the\nfield of biased facial analysis.\nLink: http://arxiv.org/abs/2112.06522v1",
        "link": "http://arxiv.org/abs/2112.06522v1"
      }
    ],
    "Machine learning for topic modeling": [
      {
        "title": "Theme and Topic: How Qualitative Research and Topic Modeling Can Be Brought Together",
        "arxiv_id": "2210.00707v1",
        "published": "2022-10-03",
        "summary": "Qualitative research is an approach to understanding social phenomenon based\naround human interpretation of data, particularly text. Probabilistic topic\nmodelling is a machine learning approach that is also based around the analysis\nof text and often is used to in order to understand social phenomena. Both of\nthese approaches aim to extract important themes or topics in a textual corpus\nand therefore we may see them as analogous to each other. However there are\nalso considerable differences in how the two approaches function. One is a\nhighly human interpretive process, the other is automated and statistical. In\nthis paper we use this analogy as the basis for our Theme and Topic system, a\ntool for qualitative researchers to conduct textual research that integrates\ntopic modelling into an accessible interface. This is an example of a more\ngeneral approach to the design of interactive machine learning systems in which\nexisting human professional processes can be used as the model for processes\ninvolving machine learning. This has the particular benefit of providing a\nfamiliar approach to existing professionals, that may can make machine learning\nseem less alien and easier to learn. Our design approach has two elements. We\nfirst investigate the steps professionals go through when performing tasks and\ndesign a workflow for Theme and Topic that integrates machine learning. We then\ndesigned interfaces for topic modelling in which familiar concepts from\nqualitative research are mapped onto machine learning concepts. This makes\nthese the machine learning concepts more familiar and easier to learn for\nqualitative researchers.\nLink: http://arxiv.org/abs/2210.00707v1",
        "link": "http://arxiv.org/abs/2210.00707v1"
      },
      {
        "title": "The Top 10 Topics in Machine Learning Revisited: A Quantitative Meta-Study",
        "arxiv_id": "1703.10121v1",
        "published": "2017-03-29",
        "summary": "Which topics of machine learning are most commonly addressed in research?\nThis question was initially answered in 2007 by doing a qualitative survey\namong distinguished researchers. In our study, we revisit this question from a\nquantitative perspective. Concretely, we collect 54K abstracts of papers\npublished between 2007 and 2016 in leading machine learning journals and\nconferences. We then use machine learning in order to determine the top 10\ntopics in machine learning. We not only include models, but provide a holistic\nview across optimization, data, features, etc. This quantitative approach\nallows reducing the bias of surveys. It reveals new and up-to-date insights\ninto what the 10 most prolific topics in machine learning research are. This\nallows researchers to identify popular topics as well as new and rising topics\nfor their research.\nLink: http://arxiv.org/abs/1703.10121v1",
        "link": "http://arxiv.org/abs/1703.10121v1"
      },
      {
        "title": "The Geometric Structure of Topic Models",
        "arxiv_id": "2403.03607v1",
        "published": "2024-03-06",
        "summary": "Topic models are a popular tool for clustering and analyzing textual data.\nThey allow texts to be classified on the basis of their affiliation to the\npreviously calculated topics. Despite their widespread use in research and\napplication, an in-depth analysis of topic models is still an open research\ntopic. State-of-the-art methods for interpreting topic models are based on\nsimple visualizations, such as similarity matrices, top-term lists or\nembeddings, which are limited to a maximum of three dimensions. In this paper,\nwe propose an incidence-geometric method for deriving an ordinal structure from\nflat topic models, such as non-negative matrix factorization. These enable the\nanalysis of the topic model in a higher (order) dimension and the possibility\nof extracting conceptual relationships between several topics at once. Due to\nthe use of conceptual scaling, our approach does not introduce any artificial\ntopical relationships, such as artifacts of feature compression. Based on our\nfindings, we present a new visualization paradigm for concept hierarchies based\non ordinal motifs. These allow for a top-down view on topic spaces. We\nintroduce and demonstrate the applicability of our approach based on a topic\nmodel derived from a corpus of scientific papers taken from 32 top machine\nlearning venues.\nLink: http://arxiv.org/abs/2403.03607v1",
        "link": "http://arxiv.org/abs/2403.03607v1"
      }
    ],
    "Systematic review automation platforms": [
      {
        "title": "A Study of Data Store-based Home Automation",
        "arxiv_id": "1812.01597v1",
        "published": "2018-12-04",
        "summary": "Home automation platforms provide a new level of convenience by enabling\nconsumers to automate various aspects of physical objects in their homes. While\nthe convenience is beneficial, security flaws in the platforms or integrated\nthird-party products can have serious consequences for the integrity of a\nuser's physical environment. In this paper we perform a systematic security\nevaluation of two popular smart home platforms, Google's Nest platform and\nPhilips Hue, that implement home automation \"routines\" (i.e., trigger-action\nprograms involving apps and devices) via manipulation of state variables in a\ncentralized data store. Our semi-automated analysis examines, among other\nthings, platform access control enforcement, the rigor of non-system\nenforcement procedures, and the potential for misuse of routines. This analysis\nresults in ten key findings with serious security implications. For instance,\nwe demonstrate the potential for the misuse of smart home routines in the Nest\nplatform to perform a lateral privilege escalation, illustrate how Nest's\nproduct review system is ineffective at preventing multiple stages of this\nattack that it examines, and demonstrate how emerging platforms may fail to\nprovide even bare-minimum security by allowing apps to arbitrarily add/remove\nother apps from the user's smart home. Our findings draw attention to the\nunique security challenges of platforms that execute routines via centralized\ndata stores and highlight the importance of enforcing security by design in\nemerging home automation platforms.\nLink: http://arxiv.org/abs/1812.01597v1",
        "link": "http://arxiv.org/abs/1812.01597v1"
      },
      {
        "title": "Can Commercial Testing Automation Tools Work for IoT? A Case Study of Selenium and Node-Red",
        "arxiv_id": "2107.04246v1",
        "published": "2021-07-09",
        "summary": "Background: Testing IoT software is challenging due to large scale, volume of\ndata and heterogeneity. Testing automation is a much-needed feature in the\ndomain. Aims: The first goal of this research is to explore the requirements\nand challenges of IoT testing automation. The second goal is to integrate\ntesting automation tools used in commercial software into the IoT context.\nMethod: A systematic literature review is carried out to elicit requirements\nfor testing automation in IoT. A design science approach is followed to build a\ntesting automation tool for IoT applications written in the Node-Red platform,\nusing the commercial testing automation tool Selenium. The resulting framework\nuses the Selenium Web Driver for browser-based testing automation for IoT\napplications. Results: The proposed framework has been functionally tested on\nmultiple browsers with preliminary evaluation on maintainability, browser\ncapability and comprehensiveness. Conclusions: The use of commercial tools for\ntesting automation in IoT is feasible. However, major challenges like high data\nvolumes and parallel transmission and processing of data need to be addressed\ncomprehensively for complete integration.\nLink: http://arxiv.org/abs/2107.04246v1",
        "link": "http://arxiv.org/abs/2107.04246v1"
      },
      {
        "title": "A Multi-Vocal Review of Security Orchestration",
        "arxiv_id": "2002.09190v1",
        "published": "2020-02-21",
        "summary": "Organizations use diverse types of security solutions to prevent\ncyberattacks. Multiple vendors provide security solutions developed using\nheterogeneous technologies and paradigms. Hence, it is a challenging rather\nimpossible to easily make security solutions to work an integrated fashion.\nSecurity orchestration aims at smoothly integrating multivendor security tools\nthat can effectively and efficiently interoperate to support security staff of\na Security Operation Centre (SOC). Given the increasing role and importance of\nsecurity orchestration, there has been an increasing amount of literature on\ndifferent aspects of security orchestration solutions. However, there has been\nno effort to systematically review and analyze the reported solutions. We\nreport a Multivocal Literature Review that has systematically selected and\nreviewed both academic and grey (blogs, web pages, white papers) literature on\ndifferent aspects of security orchestration published from January 2007 until\nJuly 2017. The review has enabled us to provide a working definition of\nsecurity orchestration and classify the main functionalities of security\norchestration into three main areas: unification, orchestration, and\nautomation. We have also identified the core components of a security\norchestration platform and categorized the drivers of security orchestration\nbased on technical and socio-technical aspects. We also provide a taxonomy of\nsecurity orchestration based on the execution environment, automation strategy,\ndeployment type, mode of task and resource type. This review has helped us to\nreveal several areas of further research and development in security\norchestration.\nLink: http://arxiv.org/abs/2002.09190v1",
        "link": "http://arxiv.org/abs/2002.09190v1"
      }
    ],
    "NLP tools for text summarization": [
      {
        "title": "Survey of Pseudonymization, Abstractive Summarization & Spell Checker for Hindi and Marathi",
        "arxiv_id": "2412.18163v1",
        "published": "2024-12-24",
        "summary": "India's vast linguistic diversity presents unique challenges and\nopportunities for technological advancement, especially in the realm of Natural\nLanguage Processing (NLP). While there has been significant progress in NLP\napplications for widely spoken languages, the regional languages of India, such\nas Marathi and Hindi, remain underserved. Research in the field of NLP for\nIndian regional languages is at a formative stage and holds immense\nsignificance. The paper aims to build a platform which enables the user to use\nvarious features like text anonymization, abstractive text summarization and\nspell checking in English, Hindi and Marathi language. The aim of these tools\nis to serve enterprise and consumer clients who predominantly use Indian\nRegional Languages.\nLink: http://arxiv.org/abs/2412.18163v1",
        "link": "http://arxiv.org/abs/2412.18163v1"
      },
      {
        "title": "Scaling Up Summarization: Leveraging Large Language Models for Long Text Extractive Summarization",
        "arxiv_id": "2408.15801v1",
        "published": "2024-08-28",
        "summary": "In an era where digital text is proliferating at an unprecedented rate,\nefficient summarization tools are becoming indispensable. While Large Language\nModels (LLMs) have been successfully applied in various NLP tasks, their role\nin extractive text summarization remains underexplored. This paper introduces\nEYEGLAXS (Easy Yet Efficient larGe LAnguage model for eXtractive\nSummarization), a framework that leverages LLMs, specifically LLAMA2-7B and\nChatGLM2-6B, for extractive summarization of lengthy text documents. Instead of\nabstractive methods, which often suffer from issues like factual inaccuracies\nand hallucinations, EYEGLAXS focuses on extractive summarization to ensure\nfactual and grammatical integrity. Utilizing state-of-the-art techniques such\nas Flash Attention and Parameter-Efficient Fine-Tuning (PEFT), EYEGLAXS\naddresses the computational and resource challenges typically associated with\nLLMs. The system sets new performance benchmarks on well-known datasets like\nPubMed and ArXiv. Furthermore, we extend our research through additional\nanalyses that explore the adaptability of LLMs in handling different sequence\nlengths and their efficiency in training on smaller datasets. These\ncontributions not only set a new standard in the field but also open up\npromising avenues for future research in extractive text summarization.\nLink: http://arxiv.org/abs/2408.15801v1",
        "link": "http://arxiv.org/abs/2408.15801v1"
      },
      {
        "title": "Open or Closed LLM for Lesser-Resourced Languages? Lessons from Greek",
        "arxiv_id": "2501.12826v1",
        "published": "2025-01-22",
        "summary": "Natural Language Processing (NLP) for lesser-resourced languages faces\npersistent challenges, including limited datasets, inherited biases from\nhigh-resource languages, and the need for domain-specific solutions. This study\naddresses these gaps for Modern Greek through three key contributions. First,\nwe evaluate the performance of open-source (Llama-70b) and closed-source\n(GPT-4o mini) large language models (LLMs) on seven core NLP tasks with dataset\navailability, revealing task-specific strengths, weaknesses, and parity in\ntheir performance. Second, we expand the scope of Greek NLP by reframing\nAuthorship Attribution as a tool to assess potential data usage by LLMs in\npre-training, with high 0-shot accuracy suggesting ethical implications for\ndata provenance. Third, we showcase a legal NLP case study, where a Summarize,\nTranslate, and Embed (STE) methodology outperforms the traditional TF-IDF\napproach for clustering \\emph{long} legal texts. Together, these contributions\nprovide a roadmap to advance NLP in lesser-resourced languages, bridging gaps\nin model evaluation, task innovation, and real-world impact.\nLink: http://arxiv.org/abs/2501.12826v1",
        "link": "http://arxiv.org/abs/2501.12826v1"
      }
    ]
  }
}